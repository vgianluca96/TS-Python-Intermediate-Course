{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f657c2-6041-4471-9164-6aa108a37157",
   "metadata": {},
   "source": [
    "#Â Laboratory Session: LangChain with OllamaLLM - Document Analyzer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a83ef7a-22af-4cb9-9d35-29573542742e",
   "metadata": {},
   "source": [
    "**Session Breakdown:**\n",
    "\n",
    "1. Setup and Environment Preparation (15 minutes)\n",
    "2. LangChain and OllamaLLM Fundamentals (30 minutes)\n",
    "3. Practical Implementation - Document Analyzer (45 minutes)\n",
    "4. Lab Wrap-Up and Discussion (15 minutes)\n",
    "\n",
    "**Resources**\n",
    "\n",
    "- [Ollama Official Documentation](https://ollama.ai/docs)\n",
    "- [LangChain Documentation](https://python.langchain.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147fc4da-2bdd-43b6-947c-baea9fbbe597",
   "metadata": {},
   "source": [
    "# LangChain Workshop: Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f704ac05-ed39-465a-a13c-82a7aade3b9c",
   "metadata": {},
   "source": [
    "### 1. Prerequisites and System Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a05de-93ff-4fa9-a4a1-8f036341f48d",
   "metadata": {},
   "source": [
    "#### Python and Environment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e036b15-59cc-47df-906d-e5192068a5d4",
   "metadata": {},
   "source": [
    "**Create venv and activate** to do with VSC assistant\n",
    "\n",
    "```bash\n",
    "python -m venv langchain_workshop # Create a virtual environment\n",
    "source langchain_workshop/bin/activate # Activate the virtual environment (On Unix or MacOS)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e14d1b-e035-4ba6-aa36-12845df4ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Python Installation\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b08ea1-9639-4d13-9231-6dee10e78d75",
   "metadata": {},
   "source": [
    "### 2. Library Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1637ac3f-8ade-4d79-9a5d-fa01ab770a79",
   "metadata": {},
   "source": [
    "#### Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361b1f04-f301-4bdb-8d73-34d1ba5be1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fd016b-ee2d-466f-a745-d2198dc8b913",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install core libraries\n",
    "!pip install \\\n",
    "    langchain \\\n",
    "    ollama \\\n",
    "    pypdf \\\n",
    "    transformers \\\n",
    "    sentence-transformers \\\n",
    "    chromadb \\\n",
    "    unstructured \\\n",
    "    langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc2eccc-b731-4a5b-9475-eff9b97f5bbc",
   "metadata": {},
   "source": [
    "### 3. Ollama and Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa8a110-b280-43e0-943a-e4828643b970",
   "metadata": {},
   "source": [
    "#### Verify Ollama Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def893bb-9908-4bd7-917c-5b68bb8a3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Ollama is running\n",
    "import ollama\n",
    "\n",
    "# List available models\n",
    "print(ollama.list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a9dea-1e14-46aa-8891-17c83cd2d829",
   "metadata": {},
   "source": [
    "#### Download Ollama Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09514bb3-e381-4d20-a489-7757a2758581",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pull a suitable open-source model\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f92e0d-ef74-4318-b7b3-5337bb365143",
   "metadata": {},
   "source": [
    "### 4. Environment Verification Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebde0db8-8083-4829-80ee-21a01b3b7f74",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import ollama\n",
    "import langchain\n",
    "\n",
    "def check_environment():\n",
    "    \"\"\"Perform comprehensive environment check.\"\"\"\n",
    "    print(\"--- Environment Check ---\\n\")\n",
    "    \n",
    "    # Python version\n",
    "    print(f\"Python Version: {sys.version}\")\n",
    "    \n",
    "    # Library versions\n",
    "    print(f\"LangChain Version: {langchain.__version__}\\n\")\n",
    "    \n",
    "    # Verify model availability\n",
    "    try:\n",
    "        models = ollama.list()\n",
    "        print(\"Ollama Models Available:\\n\")\n",
    "        for model in models['models']:\n",
    "            print(f\"- {model['model']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking Ollama models: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefecb96-0f47-4510-aebe-ce4fb128e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the check\n",
    "check_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fa1f3c-809f-4eb3-886e-b91f4c6e9261",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 5. Troubleshooting Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e44745e-2cce-4238-8ae9-def78384ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def troubleshoot_ollama():\n",
    "    \"\"\"Display Ollama troubleshooting guidelines.\"\"\"\n",
    "    print(\"\\n--- Ollama Troubleshooting Guide ---\")\n",
    "    print(\"Common Issues and Solutions:\")\n",
    "    \n",
    "    issues = {\n",
    "        \"Installation\": [\n",
    "            \"Ensure Ollama is installed\",\n",
    "            \"Check system PATH\",\n",
    "            \"Verify administrative permissions\"\n",
    "        ],\n",
    "        \"Connection\": [\n",
    "            \"Check internet connectivity\",\n",
    "            \"Verify firewall settings\",\n",
    "            \"Restart Ollama service\"\n",
    "        ],\n",
    "        \"Model Download\": [\n",
    "            \"Sufficient disk space\",\n",
    "            \"Stable internet connection\",\n",
    "            \"Use `ollama pull` command\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, solutions in issues.items():\n",
    "        print(f\"\\n{category} Troubleshooting:\")\n",
    "        for solution in solutions:\n",
    "            print(f\"- {solution}\")\n",
    "\n",
    "# Display troubleshooting guide\n",
    "troubleshoot_ollama()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1fb564-d6f3-495c-b986-e71f144b9202",
   "metadata": {},
   "source": [
    "# LangChain and Large Language Models - Theoretical Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754cbbd5-c3ec-41b2-81cb-7e2402d81b70",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. Introduction to LangChain\n",
    "2. Understanding Large Language Models\n",
    "3. Core Components of LangChain\n",
    "4. Working with Local LLMs\n",
    "5. Design Patterns and Best Practices - NO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc846e8-ae45-4bcc-b5e3-90a3df5bcc42",
   "metadata": {},
   "source": [
    "## Introduction to LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd23f6e-755c-44d6-a0d1-e6408f79d97f",
   "metadata": {},
   "source": [
    "### What is LangChain?\n",
    "\n",
    "LangChain is a powerful framework that revolutionizes how developers work with Large Language Models (LLMs). At its core, it's an orchestration layer that helps you build sophisticated applications powered by AI.\n",
    "\n",
    "#### Core Concepts\n",
    "\n",
    "##### 1. Component Architecture\n",
    "LangChain provides modular building blocks that you can combine:\n",
    "- **Prompt Templates**: Design and standardize your interactions with LLMs\n",
    "- **Model Interfaces**: Connect to various LLMs (like GPT, Ollama, etc.)\n",
    "- **Memory Systems**: Manage conversation history and context\n",
    "- **Data Connectors**: Interface with external data sources\n",
    "- **Chains**: Combine components into processing pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc633698-0ada-41f5-937b-758f03c7ebec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "##### 2. Key Features\n",
    "\n",
    "###### Chain Management\n",
    "- Create sequences of operations\n",
    "- Pass data between components\n",
    "- Handle errors and retries\n",
    "- Manage state and context\n",
    "\n",
    "###### Data Handling\n",
    "- Process various data formats\n",
    "- Transform inputs and outputs\n",
    "- Cache responses\n",
    "- Handle streaming\n",
    "\n",
    "###### Integration Capabilities\n",
    "- Connect to multiple LLM providers\n",
    "- Interface with databases\n",
    "- Work with document formats\n",
    "- Support vector stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644e14f2-783b-4f3e-832a-b2f7d82f79e4",
   "metadata": {},
   "source": [
    "<img src=\"imgs/key-features-chart.svg\" width=\"1000\" style=\"display: block; margin: 0 auto\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951682c5-6779-44ef-820e-bbe1df4326ee",
   "metadata": {},
   "source": [
    "#### Real-World Applications\n",
    "\n",
    "LangChain enables you to build:\n",
    "1. **Question-Answering Systems**\n",
    "   - Process documents\n",
    "   - Generate accurate responses\n",
    "   - Maintain context\n",
    "\n",
    "2. **Chatbots**\n",
    "   - Handle conversations\n",
    "   - Remember user interactions\n",
    "   - Process complex queries\n",
    "\n",
    "3. **Document Analysis**\n",
    "   - Extract information\n",
    "   - Generate summaries\n",
    "   - Answer specific questions\n",
    "\n",
    "4. **Code Analysis**\n",
    "   - Process source code\n",
    "   - Generate documentation\n",
    "   - Explain functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f9735-5775-4b3a-88a0-a4c651f51a26",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Benefits\n",
    "\n",
    "##### For Developers\n",
    "- Reduced development time\n",
    "- Standardized interfaces\n",
    "- Built-in best practices\n",
    "- Extensive documentation\n",
    "\n",
    "##### For Applications\n",
    "- Better scalability\n",
    "- Improved reliability\n",
    "- Consistent behavior\n",
    "- Enhanced performance\n",
    "\n",
    "This comprehensive framework simplifies complex LLM implementations while providing the flexibility needed for sophisticated applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45316e1-1b39-45fc-94c4-0e52e3a3babe",
   "metadata": {},
   "source": [
    "## Understanding Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e63b7-0de7-4c47-a00a-aa3df300ff5b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### What are Large Language Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f108e-e18a-4e4a-a85a-7e914de5f583",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) are sophisticated artificial intelligence systems that can understand, generate, and manipulate human language. Think of them as incredibly advanced pattern recognition systems that have been trained on massive amounts of text from books, websites, and other written materials.\n",
    "\n",
    "### How do LLMs Work?\n",
    "\n",
    "#### The Training Process\n",
    "1. Data Collection\n",
    "   LLMs start by \"reading\" enormous amounts of text data â billions of words from sources like books, websites, articles, and social media. This gives them exposure to how humans use language in various contexts.\n",
    "\n",
    "2. Pattern Recognition\n",
    "   During training, LLMs learn to recognize patterns in how words and phrases are used together. They develop an understanding of grammar, context, and common relationships between concepts.\n",
    "\n",
    "3. Neural Networks\n",
    "   The \"brain\" of an LLM is a neural network â a complex system of interconnected nodes inspired by how human brains work. These networks learn to process language by adjusting the strength of connections between nodes based on the patterns they observe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26b125-2ff6-4b9e-9787-36f7bee3de52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<img src=\"imgs/LLM-training-process.png\" width=\"1000\" style=\"display: block; margin: 0 auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb62c21-a659-448f-9a77-4652599e6715",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Key Characteristics\n",
    "\n",
    "#### 1. Understanding Context\n",
    "LLMs don't just look at individual words; they consider the entire context of a conversation or text. This allows them to:\n",
    "- Understand nuanced meanings\n",
    "- Recognize subtle differences in similar phrases\n",
    "- Maintain coherent conversations across multiple exchanges\n",
    "\n",
    "#### 2. Generation Capabilities\n",
    "These models can:\n",
    "- Write text that sounds natural and human-like\n",
    "- Adapt their writing style to different formats (emails, stories, technical documents)\n",
    "- Translate between languages\n",
    "- Summarize long texts into shorter versions\n",
    "\n",
    "#### 3. Knowledge Base\n",
    "Through their training, LLMs develop a broad knowledge base that includes:\n",
    "- Facts about the world\n",
    "- Common sense understanding\n",
    "- Basic reasoning abilities\n",
    "- Cultural references and contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ab4971-9dfa-4ff8-8bc5-4cce14fc7a0a",
   "metadata": {},
   "source": [
    "### What LLMs Can't Do\n",
    "\n",
    "- They don't truly \"understand\" in the way humans do\n",
    "- They can't learn from conversations in real-time\n",
    "- They don't have genuine emotions or consciousness\n",
    "- They can sometimes make mistakes or generate incorrect information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf18e32-b694-431a-a9ac-3f588c1d819e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Types of LLMs\n",
    "1. **Cloud-based Models**\n",
    "   - OpenAI GPT Series\n",
    "   - Anthropic Claude\n",
    "   - Google PaLM\n",
    "\n",
    "2. **Local Models**\n",
    "   - Ollama Models\n",
    "   - Llama 2\n",
    "   - GPT4All\n",
    "   - LocalAI\n",
    "\n",
    "### Comparing Local vs. Cloud LLMs\n",
    "\n",
    "| Aspect | Local LLMs | Cloud LLMs |\n",
    "|--------|------------|------------|\n",
    "| Privacy | High | Depends on Provider |\n",
    "| Cost | One-time/Free | Pay-per-use |\n",
    "| Latency | Hardware Dependent | Network Dependent |\n",
    "| Setup Complexity | Higher | Lower |\n",
    "| Customization | More Flexible | Limited |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b33373-d51a-443c-b550-64d7961fbf3c",
   "metadata": {},
   "source": [
    "## Core Components of LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93582b78-d13e-4d8f-bc24-9ca024ef546f",
   "metadata": {},
   "source": [
    "### 1. Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a591eb1-9289-42b0-9b89-fd0b1ad2954b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Chains are sequences of operations that:\n",
    "- Process inputs systematically\n",
    "- Combine multiple components\n",
    "- Manage state and memory\n",
    "- Handle errors and edge cases\n",
    "\n",
    "Example Chain Structure:\n",
    "```python\n",
    "input â Prompt Template â LLM â Output Parser â final output\n",
    "```\n",
    "\n",
    "**Basic Chain Syntax**\n",
    "```python\n",
    "chain = prompt | self.llm\n",
    "response = chain.invoke({\n",
    "    'context': context,\n",
    "    'question': question\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b8541b-6182-498d-a0cd-944c64141a9e",
   "metadata": {},
   "source": [
    "#### Step-by-Step Breakdown\n",
    "\n",
    "##### 1. Chain Creation (`chain = prompt | self.llm`)\n",
    "- The `|` (pipe) operator creates a sequential chain\n",
    "- Purpose: \"Take the output of the prompt template and feed it into the LLM\"\n",
    "- Similar to Unix pipes: output of one command becomes input to another\n",
    "- Under the hood process:\n",
    "  1. Format prompt template with variables\n",
    "  2. Send formatted prompt to LLM for processing\n",
    "\n",
    "##### 2. Prompt Template Definition\n",
    "```python\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Invoice Data:\\n{context}\\n\\n\"\n",
    "    \"Question: {question}\\n\\n\"\n",
    "    \"Provide a detailed, data-driven answer.\"\n",
    ")\n",
    "```\n",
    "- Template contains two variables: `{context}` and `{question}`\n",
    "- Variables are filled with actual values during invocation\n",
    "\n",
    "##### 3. LLM Configuration\n",
    "```python\n",
    "self.llm = Ollama(model='mistral', temperature=0.1)\n",
    "```\n",
    "\n",
    "##### 4. Chain Invocation\n",
    "```python\n",
    "response = chain.invoke({\n",
    "    'context': context,\n",
    "    'question': question\n",
    "})\n",
    "```\n",
    "Process flow:\n",
    "1. Takes input dictionary\n",
    "2. Formats prompt template\n",
    "3. Sends to Ollama\n",
    "4. Returns response\n",
    "\n",
    "#### Example Usage\n",
    "\n",
    "```python\n",
    "# Context and question\n",
    "context = \"Invoice #1: $500, Invoice #2: $300\"\n",
    "question = \"What's the total amount?\"\n",
    "\n",
    "# Generated prompt\n",
    "formatted_prompt = \"\"\"\n",
    "Invoice Data:\n",
    "Invoice #1: $500, Invoice #2: $300\n",
    "\n",
    "Question: What's the total amount?\n",
    "\n",
    "Provide a detailed, data-driven answer.\n",
    "\"\"\"\n",
    "\n",
    "chain = prompt | self.llm\n",
    "response = chain.invoke({'context': context, 'question': question})\n",
    "\n",
    "# Example response\n",
    "response = \"The total amount across the two invoices is $800, calculated by adding Invoice #1 ($500) and Invoice #2 ($300).\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2e8674-c4f0-4623-a014-1eb017f1269e",
   "metadata": {},
   "source": [
    "### 2. Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecba24a-d090-4f90-bda0-6d5e88cd6723",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Prompts in LangChain are sophisticated templates that shape how LLMs understand and respond to requests. Think of them as smart instruction sets that combine fixed text with dynamic variables to generate consistent and targeted responses.\n",
    "\n",
    "Prompts are structured templates that:\n",
    "- Guide LLM behavior\n",
    "- Include context and instructions\n",
    "- Handle variable substitution\n",
    "- Enforce output formats\n",
    "\n",
    "#### Core Components of a Prompt\n",
    "\n",
    "##### 1. Base Template\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Given the following information about a customer:\n",
    "Name: {customer_name}\n",
    "Purchase History: {purchase_history}\n",
    "\n",
    "Please provide a personalized product recommendation.\n",
    "Focus on their buying patterns and potential needs.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "##### 2. Variable Placeholders\n",
    "- Enclosed in curly braces: `{variable_name}`\n",
    "- Dynamically filled at runtime\n",
    "- Can include multiple variables\n",
    "- Support complex data structures\n",
    "\n",
    "##### 3. Instructions\n",
    "```python\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please analyze this financial data:\n",
    "    {financial_data}\n",
    "    \n",
    "    Instructions:\n",
    "    1. Identify key trends\n",
    "    2. Calculate growth rates\n",
    "    3. Flag any anomalies\n",
    "    4. Provide actionable insights\n",
    "    \n",
    "    Format your response as bullet points.\n",
    "    \"\"\"\n",
    ")\n",
    "```\n",
    "\n",
    "#### Types of Prompts\n",
    "\n",
    "##### 1. Simple Prompts\n",
    "```python\n",
    "basic_prompt = PromptTemplate(\n",
    "    template=\"What is the capital of {country}?\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "```\n",
    "\n",
    "##### 2. Structured Prompts\n",
    "```python\n",
    "analysis_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    \n",
    "    Provide a detailed answer that:\n",
    "    - References specific data points\n",
    "    - Explains the reasoning\n",
    "    - Includes relevant examples\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "```\n",
    "\n",
    "##### 3. Few-Shot Prompts\n",
    "```python\n",
    "few_shot_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Example 1:\n",
    "    Input: High fever, cough\n",
    "    Output: Possible flu, seek medical attention\n",
    "    \n",
    "    Example 2:\n",
    "    Input: Headache, fatigue\n",
    "    Output: Could be stress, rest recommended\n",
    "    \n",
    "    New Case:\n",
    "    Input: {symptoms}\n",
    "    Output:\n",
    "    \"\"\",\n",
    "    input_variables=[\"symptoms\"]\n",
    ")\n",
    "```\n",
    "\n",
    "#### Output Control\n",
    "\n",
    "##### 1. Format Specification\n",
    "```python\n",
    "formatted_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Analyze this sales data: {data}\n",
    "    \n",
    "    Return the analysis in the following JSON format:\n",
    "    {\n",
    "        \"total_sales\": number,\n",
    "        \"top_products\": [string],\n",
    "        \"growth_rate\": percentage\n",
    "    }\n",
    "    \"\"\",\n",
    "    input_variables=[\"data\"]\n",
    ")\n",
    "```\n",
    "\n",
    "##### 2. Response Guidelines\n",
    "```python\n",
    "guided_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Review this code: {code}\n",
    "    \n",
    "    Provide feedback in these categories:\n",
    "    1. Security Issues\n",
    "    2. Performance Optimizations\n",
    "    3. Code Style\n",
    "    4. Potential Bugs\n",
    "    \n",
    "    For each category, list specific findings and recommendations.\n",
    "    \"\"\",\n",
    "    input_variables=[\"code\"]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a340b00d-874a-4a0d-9477-f044bbcda06e",
   "metadata": {},
   "source": [
    "#### Best Practices\n",
    "\n",
    "##### 1. Clarity\n",
    "- Use clear, specific instructions\n",
    "- Break down complex tasks\n",
    "- Provide examples when needed\n",
    "\n",
    "##### 2. Structure\n",
    "- Organize information logically\n",
    "- Use formatting for readability\n",
    "- Include section headers\n",
    "\n",
    "##### 3. Variables\n",
    "- Use descriptive variable names\n",
    "- Validate input data\n",
    "- Handle missing values\n",
    "\n",
    "##### 4. Output Control\n",
    "- Specify desired format\n",
    "- Include validation criteria\n",
    "- Set clear expectations\n",
    "\n",
    "**Practical Example**\n",
    "```python\n",
    "invoice_analysis_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Analyze this invoice data:\n",
    "    {invoice_data}\n",
    "    \n",
    "    Provide:\n",
    "    1. Total revenue calculation\n",
    "    2. Payment status summary\n",
    "    3. Customer payment patterns\n",
    "    4. Risk assessment\n",
    "    \n",
    "    Format as a business report with sections and bullet points.\n",
    "    Include specific numbers and percentages.\n",
    "    Flag any concerning patterns.\n",
    "    \"\"\",\n",
    "    input_variables=[\"invoice_data\"]\n",
    ")\n",
    "```\n",
    "**Remember**: A well-crafted prompt is the foundation of reliable and accurate LLM responses. Take time to design prompts that clearly communicate your requirements and constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0daa92a-3d8f-4da2-9da2-c0f40b81b4a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3. Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63cc0f4-94ff-4741-a4fc-c408f975bbf4",
   "metadata": {},
   "source": [
    "Memory systems in LangChain:\n",
    "- Store conversation history\n",
    "- Maintain context\n",
    "- Handle token limitations\n",
    "- Enable stateful interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ca8d5-a22e-4d90-a0f4-1a79f9080dc2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4. Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f6c632-a630-4c26-9821-1c0c09a6e51b",
   "metadata": {},
   "source": [
    "Output parsers:\n",
    "- Structure LLM responses\n",
    "- Validate outputs\n",
    "- Transform data formats\n",
    "- Handle errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d567fb-b2e6-4529-b864-0506a400b4eb",
   "metadata": {},
   "source": [
    "## Working with Local LLMs: Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668e9ef4-5805-4ab5-9f30-a1f72faf732c",
   "metadata": {},
   "source": [
    "#### What is Ollama?\n",
    "Ollama is an open-source tool that simplifies running Large Language Models (LLMs) locally on your machine. It provides:\n",
    "- Easy model management\n",
    "- Local execution without cloud dependencies\n",
    "- Command-line and API interfaces\n",
    "- Support for multiple models\n",
    "- Custom model support\n",
    "- API compatibility\n",
    "\n",
    "#### How Ollama Works\n",
    "\n",
    "##### 1. Model Management\n",
    "```bash\n",
    "# List available models\n",
    "ollama list\n",
    "\n",
    "# Pull a model\n",
    "ollama pull mistral\n",
    "\n",
    "# Remove a model\n",
    "ollama rm mistral\n",
    "```\n",
    "\n",
    "##### 2. Model Storage\n",
    "- Models are stored locally at:\n",
    "  - macOS: `~/.ollama/models`\n",
    "  - Linux: `~/.ollama/models`\n",
    "  - Windows: `C:\\Users\\[User]\\.ollama\\models`\n",
    "\n",
    "##### 3. Available Models\n",
    "Popular models include:\n",
    "- Mistral\n",
    "- Llama 2\n",
    "- CodeLlama\n",
    "- Phi-2\n",
    "- Neural Chat\n",
    "\n",
    "#### Using Ollama\n",
    "\n",
    "##### 1. Command Line Interface\n",
    "```bash\n",
    "# Basic usage\n",
    "ollama run mistral \"What is the capital of France?\"\n",
    "\n",
    "# Interactive chat\n",
    "ollama run mistral\n",
    "```\n",
    "\n",
    "##### 2. Python Integration\n",
    "```python\n",
    "import ollama\n",
    "\n",
    "# Create a conversation\n",
    "response = ollama.chat(model='mistral', \n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'What is the capital of France?'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])\n",
    "```\n",
    "\n",
    "##### 3. Model Parameters\n",
    "```python\n",
    "# Configure model behavior\n",
    "response = ollama.generate(\n",
    "    model='mistral',\n",
    "    prompt='Write a poem about AI',\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_predict=100\n",
    ")\n",
    "```\n",
    "\n",
    "#### Integration with LangChain\n",
    "\n",
    "```python\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# Simple generation\n",
    "response = llm.invoke(\"Write a hello world program in Python\")\n",
    "\n",
    "# With parameters\n",
    "llm = Ollama(\n",
    "    model=\"mistral\",\n",
    "    temperature=0.1,\n",
    "    num_ctx=4096\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504222ae-a692-4874-afe9-4063ea3891c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c256afc4-7458-4f0d-a932-a7fd46334e06",
   "metadata": {},
   "source": [
    "##### Common Issues\n",
    "1. **Model Not Found**\n",
    "```bash\n",
    "ollama pull mistral  # Re-download model\n",
    "```\n",
    "\n",
    "2. **Service Not Running**\n",
    "```bash\n",
    "ollama serve  # Start service\n",
    "```\n",
    "\n",
    "3. **Memory Issues**\n",
    "- Reduce context length\n",
    "- Close unnecessary applications\n",
    "- Consider smaller models\n",
    "\n",
    "##### System Requirements\n",
    "- Minimum 8GB RAM\n",
    "- 4GB free disk space per model\n",
    "- x86_64 or ARM64 processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76b3bd4-d339-4a6a-aa7b-defaa2aa9e0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Design Patterns and Best Practices NO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4812fc-c5c7-4a12-9187-886a0ddd924d",
   "metadata": {},
   "source": [
    "### 1. Prompt Engineering Patterns\n",
    "- **Zero-shot Learning**: No examples needed\n",
    "- **Few-shot Learning**: Include examples in prompt\n",
    "- **Chain-of-Thought**: Break down complex reasoning\n",
    "- **Self-Consistency**: Multiple passes for verification\n",
    "\n",
    "### 2. Error Handling\n",
    "```python\n",
    "try:\n",
    "    response = chain.invoke(input_data)\n",
    "except LLMError:\n",
    "    # Handle model errors\n",
    "except ChainError:\n",
    "    # Handle chain execution errors\n",
    "except ParseError:\n",
    "    # Handle output parsing errors\n",
    "```\n",
    "\n",
    "### 3. Performance Optimization\n",
    "- Cache frequently used results\n",
    "- Batch similar requests\n",
    "- Implement retry mechanisms\n",
    "- Monitor token usage\n",
    "\n",
    "### 4. Security Considerations\n",
    "1. Input Validation\n",
    "2. Output Sanitization\n",
    "3. Rate Limiting\n",
    "4. Access Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d90f6-2ba6-4df6-97af-8e978f53663f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c4b5e-fc35-4f1a-836e-d9efd5d35030",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "1. LangChain Documentation\n",
    "   - [Python Documentation](https://python.langchain.com/)\n",
    "   - [JavaScript Documentation](https://js.langchain.com/)\n",
    "\n",
    "2. Ollama Resources\n",
    "   - [Official Documentation](https://ollama.ai/docs)\n",
    "   - [Model Library](https://ollama.ai/library)\n",
    "\n",
    "3. Related Topics\n",
    "   - Prompt Engineering\n",
    "   - Vector Databases\n",
    "   - Embeddings\n",
    "   - RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228550cf-106f-48be-a2ac-65ce31b21c97",
   "metadata": {},
   "source": [
    "# LangChain Invoice Analyzer Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e4a14d-0897-4c7f-8116-8234c23d0fe6",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Create an invoice analysis tool that can:\n",
    "- Load JSON invoice data\n",
    "- Perform financial analytics\n",
    "- Generate insights\n",
    "- Answer specific questions about invoices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3abbf1-4c57-47c1-9866-414912d5a15e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Sample Invoice JSON Structure\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"invoice_id\": \"INV-2024-001\",\n",
    "        \"customer_name\": \"TechCorp Solutions\",\n",
    "        \"date\": \"2024-01-15\",\n",
    "        \"total_amount\": 5750.25,\n",
    "        \"items\": [\n",
    "            {\"name\": \"Software License\", \"quantity\": 10, \"unit_price\": 450.00},\n",
    "            {\"name\": \"Cloud Services\", \"quantity\": 5, \"unit_price\": 250.50}\n",
    "        ],\n",
    "        \"payment_status\": \"Paid\",\n",
    "        \"tax_rate\": 0.18\n",
    "    },\n",
    "    {\n",
    "        ...\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a85fc2-81bd-407a-a87e-102402860ee6",
   "metadata": {},
   "source": [
    "## Task 1: Implement AI-Powered Financial Insights Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c5124-c472-42e3-ae4c-40287c321222",
   "metadata": {},
   "source": [
    "Given the class AdvancedInvoiceAnalyzer, create a method that generates AI-powered insights from invoice data using LangChain and Ollama. The method should compile financial data and use an LLM to provide business analysis.\n",
    "\n",
    "### Method Structure\n",
    "```python\n",
    "def ai_powered_insights(self):\n",
    "    \"\"\"Your implementation here\"\"\"\n",
    "```\n",
    "\n",
    "### The method should:\n",
    "1. Combine data from three existing methods:\n",
    "   - `financial_summary()`\n",
    "   - `generate_tax_analysis()`\n",
    "   - `item_level_analysis()`\n",
    "\n",
    "2. Create a structured context string containing:\n",
    "   - Financial Overview\n",
    "   - Tax Analysis\n",
    "   - Top Items Analysis\n",
    "\n",
    "3. Use LangChain's PromptTemplate to create an analysis prompt\n",
    "\n",
    "4. Return AI-generated insights using the LLM\n",
    "\n",
    "### Expected Format of Context String\n",
    "```text\n",
    "Financial Overview:\n",
    "- Total Invoices: [number]\n",
    "- Total Revenue: $[amount]\n",
    "- Payment Status: [breakdown]\n",
    "\n",
    "Tax Analysis:\n",
    "- Total Tax Collected: $[amount]\n",
    "- Average Tax Rate: [percentage]\n",
    "\n",
    "Top Items:\n",
    "[item analysis data]\n",
    "```\n",
    "\n",
    "### Steps to Complete\n",
    "1. Collect data from existing analysis methods\n",
    "2. Format the context string with f-strings\n",
    "3. Create a PromptTemplate for analysis\n",
    "4. Set up LangChain chain\n",
    "5. Return the insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c811b3-063c-429b-b2c5-39830c250a41",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Task 2: Implement Interactive Query Handler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f72abd-e1b8-49d7-959d-12178f256bfd",
   "metadata": {},
   "source": [
    "### Your Task\n",
    "Create a method that allows users to ask questions about invoice data and receive AI-generated answers. The method should take a user question, combine it with invoice data context, and use an LLM to generate a response.\n",
    "\n",
    "### Method Structure\n",
    "```python\n",
    "def interactive_query(self, question):\n",
    "    \"\"\"Your implementation here\"\"\"\n",
    "```\n",
    "\n",
    "### Method Parameters\n",
    "- `question`: String containing the user's query about the invoices\n",
    "\n",
    "### The method should:\n",
    "1. Create a prompt template with context and question\n",
    "2. Generate a response using the LLM\n",
    "\n",
    "### Expected Prompt Format\n",
    "```text\n",
    "Invoice Data:\n",
    "[dataframe contents]\n",
    "\n",
    "Question: [user's question]\n",
    "\n",
    "Provide a detailed, data-driven answer. \n",
    "If the question cannot be directly answered, explain why.\n",
    "```\n",
    "\n",
    "### Steps to Complete\n",
    "1. Create PromptTemplate with two variables:\n",
    "   - `context`\n",
    "   - `question`\n",
    "2. Set up LangChain chain\n",
    "3. Return the answer\n",
    "\n",
    "### Sample Usage\n",
    "```python\n",
    "# Example usage\n",
    "analyzer = AdvancedInvoiceAnalyzer(data)\n",
    "response = analyzer.interactive_query(\"What is the total revenue?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9dbec0-150f-4548-8366-6473093f5770",
   "metadata": {},
   "source": [
    "## Your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e8aa5-e65f-41b0-9348-90457bf93402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd95889-ada2-44a8-b4e2-7f641eb69c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedInvoiceAnalyzer:\n",
    "    def __init__(self, invoices_data, model):\n",
    "        \"\"\"\n",
    "        Initialize the invoice analyzer with comprehensive data processing.\n",
    "        \n",
    "        :param invoices_data: List of invoice dictionaries\n",
    "        :param model: Name of the model to use\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame with expanded item details\n",
    "        self.df = self._prepare_dataframe(invoices_data)\n",
    "        \n",
    "        # Setup Ollama Language Model\n",
    "        self.llm = Ollama(model=model, temperature=0.1)\n",
    "    \n",
    "    def _prepare_dataframe(self, invoices_data):\n",
    "        \"\"\"\n",
    "        Prepare a comprehensive DataFrame with expanded invoice details.\n",
    "        \n",
    "        :param invoices_data: List of invoice dictionaries\n",
    "        :return: Processed pandas DataFrame\n",
    "        \"\"\"\n",
    "        # Flatten the invoices to include item-level details\n",
    "        flattened_invoices = []\n",
    "        for invoice in invoices_data:\n",
    "            base_invoice = invoice.copy()\n",
    "            for item in invoice['items']:\n",
    "                invoice_item = base_invoice.copy()\n",
    "                invoice_item.update(item)\n",
    "                invoice_item['item_total'] = item['quantity'] * item['unit_price']\n",
    "                flattened_invoices.append(invoice_item)\n",
    "        \n",
    "        return pd.DataFrame(flattened_invoices)\n",
    "    \n",
    "    def financial_summary(self):\n",
    "        \"\"\"\n",
    "        Generate comprehensive financial summary.\n",
    "        \n",
    "        :return: Dictionary of financial insights\n",
    "        \"\"\"\n",
    "        summary = {\n",
    "            'total_invoices': len(self.df['invoice_id'].unique()),\n",
    "            'total_revenue': self.df['total_amount'].sum(),\n",
    "            'average_invoice_value': self.df['total_amount'].mean(),\n",
    "            'payment_status_breakdown': self.df['payment_status'].value_counts().to_dict(),\n",
    "            'top_customers': (self.df.groupby('customer_name')['total_amount']\n",
    "                               .sum()\n",
    "                               .nlargest(3)\n",
    "                               .to_dict())\n",
    "        }\n",
    "        return summary\n",
    "    \n",
    "    def generate_tax_analysis(self):\n",
    "        \"\"\"\n",
    "        Analyze tax implications across invoices.\n",
    "        \n",
    "        :return: Detailed tax analysis\n",
    "        \"\"\"\n",
    "        tax_summary = {\n",
    "            'total_tax_collected': (self.df['total_amount'] * self.df['tax_rate']).sum(),\n",
    "            'average_tax_rate': self.df['tax_rate'].mean(),\n",
    "            'tax_rate_distribution': self.df['tax_rate'].value_counts().to_dict()\n",
    "        }\n",
    "        return tax_summary\n",
    "    \n",
    "    def item_level_analysis(self):\n",
    "        \"\"\"\n",
    "        Perform detailed analysis of invoice items.\n",
    "        \n",
    "        :return: Comprehensive item-level insights\n",
    "        \"\"\"\n",
    "        item_summary = (self.df.groupby('name').agg({\n",
    "            'quantity': 'sum',\n",
    "            'item_total': 'sum',\n",
    "            'unit_price': 'mean'\n",
    "        }).sort_values('item_total', ascending=False))\n",
    "        \n",
    "        return item_summary\n",
    "    \n",
    "    def ai_powered_insights(self):\n",
    "        \"\"\"\n",
    "        Generate AI-powered natural language insights.\n",
    "        \n",
    "        :return: Conversational financial analysis\n",
    "        \"\"\"\n",
    "        # Prepare comprehensive context\n",
    "        context = f\"\"\"\n",
    "        Financial Overview:\n",
    "        - Total Invoices: {...}\n",
    "        - Total Revenue: ${...}\n",
    "        - Payment Status: {...}\n",
    "        \n",
    "        Tax Analysis:\n",
    "        - Total Tax Collected: ${...}\n",
    "        - Average Tax Rate: {...}\n",
    "        \n",
    "        Top Items:\n",
    "        {...}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create a prompt template for generating insights\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"...\"\n",
    "            \"...\"\n",
    "            \"...\"\n",
    "        )\n",
    "        \n",
    "        # Create a chain with the LLM\n",
    "        chain = ...\n",
    "        return chain.invoke(...)\n",
    "    \n",
    "    def interactive_query(self, question):\n",
    "        \"\"\"\n",
    "        Answer specific questions about the invoices.\n",
    "        \n",
    "        :param question: User's specific query\n",
    "        :return: Analytical response\n",
    "        \"\"\"\n",
    "        # Prepare context with all invoice details\n",
    "        context = self.df.to_string()\n",
    "        \n",
    "        # Create a flexible query prompt\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"\"\n",
    "            \"\"\n",
    "            ...\n",
    "        )\n",
    "        \n",
    "        # Create a chain with the LLM\n",
    "        chain = ...\n",
    "        return chain.invoke(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3747d2-d107-4837-b16f-0d03b6e8f382",
   "metadata": {},
   "source": [
    "## Test Your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3803d028-927b-4972-8153-bffb4c901a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load invoices from JSON file\n",
    "with open('data/invoice-data.json', 'r') as file:\n",
    "    invoices = json.load(file)\n",
    "\n",
    "# Initialize the analyzer\n",
    "analyzer = AdvancedInvoiceAnalyzer(invoices, model='llama3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d6a4a-4337-4d53-9ce2-1db9589a412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Financial Summary\n",
    "financial_summary = analyzer.financial_summary()\n",
    "\n",
    "print(\"Financial Summary:\")\n",
    "print(json.dumps(financial_summary, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba88450b-d536-4335-83ef-203bbbfac390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tax Analysis\n",
    "tax_analysis = analyzer.generate_tax_analysis()\n",
    "print(\"Tax Analysis:\")\n",
    "print(json.dumps(tax_analysis, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16616359-3dc9-472e-a284-aff98a0688aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item-Level Analysis\n",
    "item_level_analysis = analyzer.item_level_analysis()\n",
    "print(\"Item-Level Analysis:\")\n",
    "print(item_level_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c021b2-970e-4844-81a4-24d0373b8e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_level_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42161bc3-69a1-4221-949d-dfa89fb5e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI-Powered Insights\n",
    "ai_powered_insights = analyzer.ai_powered_insights()\n",
    "print(\"\\nAI-Powered Insights:\")\n",
    "print(ai_powered_insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6252468d-319c-4d88-979b-7bfe8faf9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive query\n",
    "print(\"Interactive Query:\")\n",
    "question = input()\n",
    "print(\"i'm thinking...\")\n",
    "answer = analyzer.interactive_query(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89333cb9-9848-4551-8e47-d2ffc3319829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9e0993-9445-4603-87dc-d341ac9e7d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
